% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/data-importing.R
\name{get_reddit_content}
\alias{get_reddit_content}
\title{Scrape Content from Multiple URLs and combine into a single data frame}
\usage{
get_reddit_content(content_type, limit, ...)
}
\arguments{
\item{content_type}{Type of content you're looking for. Permissible values
are "comment" and "submission". Defaults to "comment". This argument is
passed to \code{construct_pushshift_url}.}

\item{limit}{Max number of results to return. Defaults to 500, which is the
maximum number of results that can be returned from a single pushshift
api query. Please keep in mind your available system resources and any
potential burden on other servers when determining the number of rows
you need.}

\item{...}{Additional arguments to pass to \code{construct_pushshift_url} to
build api query.}
}
\value{
A data.frame with content imported from your query
}
\description{
This function dynamically constructs URLs and imports the
data contained within them. It is designed to automate the process of
importing larger amounts of data.
}
